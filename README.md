# PARTE 1: Arquitectura

<img width="1781" height="900" alt="arquitectura_datos_reto-PaÌgina-4 drawio" src="https://github.com/user-attachments/assets/5428fe76-1da0-4ea3-8c2b-3b90a002668b" />

## 1. SelecciÃ³n de Componentes

### Ingesta de Datos

**Eventos (Streaming)**
- Kinesis Data Streams con Firehose porque necesitamos capturar eventos en tiempo real y ademÃ¡s mantener un buffer para reprocesamiento. La retenciÃ³n de 7 dÃ­as nos da margen para corregir errores en la lÃ³gica de transformaciÃ³n sin perder datos.
- Firehose convierte automÃ¡ticamente de JSON a Parquet y particiona por fecha, lo que nos ahorra escribir cÃ³digo de conversiÃ³n y reduce significativamente el costo de storage en S3 (compresiÃ³n snappy).

**Transacciones (Batch)**
- EventBridge + Lambda porque los archivos CSV llegan cada hora desde un SFTP externo. Con Lambda solo pagamos por las 720 ejecuciones mensuales (~$2), mientras que un servidor SFTP administrado (Transfer Family) nos costarÃ­a $220/mes constantes.
- La Lambda descarga el archivo, hace validaciones bÃ¡sicas (schema, nulls) y sube a S3. Simple y efectivo.

**Usuarios (Dimensional)**
- Lambda con extracciÃ³n incremental diaria. La tabla de usuarios no cambia mucho (quizÃ¡s un 2-3% diario con nuevos registros), asÃ­ que no tiene sentido pagar $145/mes por DMS cuando Lambda nos cuesta $3/mes.
- Usamos watermarking en DynamoDB para trackear el Ãºltimo `updated_at` procesado y solo extraer deltas. La Lambda estÃ¡ en la VPC para conectarse a RDS de forma segura.

### Storage

**S3 + Iceberg**

La decisiÃ³n de usar Iceberg sobre Parquet plano viene de experiencia previa lidiando con datos que llegan tarde. Con Parquet tendrÃ­as que:
- Leer toda la particiÃ³n
- Hacer merge en memoria
- Reescribir todo el archivo

Con Iceberg hacemos un `MERGE` SQL y el engine se encarga de la complejidad. AdemÃ¡s nos da snapshots para rollback si metemos la pata en alguna transformaciÃ³n.

El particionamiento por `days(session_date)` es hidden, asÃ­ que los analistas no tienen que acordarse de incluir la particiÃ³n en sus queries. Iceberg lo optimiza automÃ¡ticamente.

### Procesamiento

**Glue ETL**

NecesitÃ¡bamos Spark para procesar los datos y Glue nos evita gestionar un cluster EMR. Los jobs se facturan por DPU-hora, y con las transformaciones que tenemos (joins, agregaciones, deduplicaciÃ³n) estamos gastando ~40 DPU-horas al mes ($100).

Evaluamos usar Lambda para todo, pero Lambda tiene lÃ­mite de 15 minutos y memoria de 10GB. Para procesar millones de eventos y hacer joins complejos, Spark es mÃ¡s apropiado.

**Step Functions**

Para orquestar los jobs usamos Step Functions porque nuestro pipeline es bastante lineal:
1. Tres jobs Bronzeâ†’Silver en paralelo
2. Un job Silverâ†’Gold que depende de los tres anteriores
3. NotificaciÃ³n

No necesitamos todas las features de Airflow (pools, custom operators, SLAs complejos). Step Functions nos cuesta literalmente centavos y se integra nativamente con Glue usando `.sync` para esperar a que termine cada job.

Si en el futuro el pipeline se complica mucho, podemos migrar a Airflow self-hosted en EC2 (~$50/mes) o evaluar MWAA si justifica el costo.

### Red

**VPC + Endpoint S3**

La Lambda que extrae de RDS tiene que estar en la VPC porque RDS estÃ¡ en subnets privadas (security best practice). El tema es que Lambda en VPC por default no puede acceder a internet, entonces para subir a S3 tienes dos opciones:
- NAT Gateway: $32/mes + $0.045/GB
- VPC Endpoint: $7/mes + $0.01/GB

Elegimos VPC Endpoint porque con ~100GB/mes de uploads sale mucho mÃ¡s barato.

El trade-off es que Lambda en VPC tiene cold start mÃ¡s lento (10-15 seg vs 1 seg), pero como nuestros jobs son diarios/hourly, no es crÃ­tico.

## 2. Esquema de la Tabla Gold
```sql
CREATE TABLE gold.user_session_analysis (
    user_id STRING,
    session_id STRING,
    
    -- Datos del usuario (snapshot al momento de la sesiÃ³n)
    signup_date DATE,
    device_type STRING,
    country STRING,
    
    -- MÃ©tricas de la sesiÃ³n
    session_start_time TIMESTAMP,
    session_end_time TIMESTAMP,
    session_duration_seconds INT,
    session_date DATE,  -- Para particionar
    
    -- Agregados de eventos
    total_events INT,
    event_types ARRAY<STRING>,
    first_event_type STRING,
    last_event_type STRING,
    
    -- Datos de transacciÃ³n (nullable porque no todas las sesiones compran)
    transaction_id STRING,
    transaction_amount DECIMAL(10,2),
    transaction_currency STRING,
    transaction_timestamp TIMESTAMP,
    
    -- Flags calculados
    has_transaction BOOLEAN,
    time_to_purchase_seconds INT,
    
    -- Metadata
    processing_timestamp TIMESTAMP,
    data_version INT,
    source_system STRING
)
USING iceberg
PARTITIONED BY (days(session_date))
TBLPROPERTIES (
    'write.format.default' = 'parquet',
    'write.parquet.compression-codec' = 'snappy',
    'format-version' = '2',
    'write.merge.mode' = 'merge-on-read'
)
LOCATION 's3://digital-services-datalake/gold/user_session_analysis/';
```

**Decisiones de diseÃ±o:**

- `ARRAY<STRING>` para `event_types` porque queremos mantener el orden temporal de los eventos en la sesiÃ³n. Es mÃ¡s flexible para anÃ¡lisis de funnel que tener columnas separadas.

- Los campos de transacciÃ³n son nullable con LEFT JOIN desde transactions. Preferimos esto a tener dos tablas separadas (sessions con compra vs sin compra) porque simplifica las queries analÃ­ticas.

- `session_date` es derivado de `session_start_time` y se usa para particionar. Iceberg lo hace transparente - el usuario no tiene que saber que estÃ¡ particionado.

- `DECIMAL(10,2)` para amounts porque floats dan problemas de precisiÃ³n con dinero. Aprendimos esto de la forma difÃ­cil.

- `data_version` por si necesitamos cambiar la lÃ³gica de agregaciÃ³n en el futuro y queremos saber quÃ© registros usan quÃ© versiÃ³n.

## 3. Manejo de Escenarios

### Late-arriving data

El problema ocurre cuando un evento se genera a las 10 AM pero por problemas de red llega a Kinesis a las 4 PM. Para ese momento ya corrimos el pipeline de la maÃ±ana y la sesiÃ³n estÃ¡ incompleta en la tabla gold.

**SoluciÃ³n prÃ¡ctica:**

1. Kinesis retiene 7 dÃ­as. Si detectamos el problema, podemos reprocesar desde un timestamp especÃ­fico usando `GetShardIterator` con `AT_TIMESTAMP`.

2. El job Silverâ†’Gold hace `MERGE` en lugar de `INSERT`:
```sql
MERGE INTO gold.user_session_analysis t
USING new_sessions s
ON t.session_id = s.session_id AND t.user_id = s.user_id
WHEN MATCHED THEN UPDATE SET *
WHEN NOT MATCHED THEN INSERT *
```

Esto es idempotente. Si una sesiÃ³n ya existe, la actualizamos con los datos mÃ¡s completos. Si es nueva, la insertamos.

3. Corremos el pipeline con una ventana de lookback. Por ejemplo, cada vez que procesamos, miramos los Ãºltimos 2 dÃ­as de datos en Bronze, no solo el dÃ­a actual. Esto captura eventos que llegaron tarde.

El costo de reprocesar es bajo porque Iceberg solo reescribe las particiones afectadas, no toda la tabla.

### Calidad de datos

**Duplicados:**

En cada capa tenemos deduplicaciÃ³n:

Bronzeâ†’Silver:
```python
window = Window.partitionBy("event_id").orderBy(col("event_timestamp").desc())
df_clean = df.withColumn("rn", row_number().over(window)) \
             .filter(col("rn") == 1) \
             .drop("rn")
```

Esto se basa en que `event_id` es Ãºnico. Si llega el mismo evento dos veces (retry del producer), nos quedamos con el mÃ¡s reciente.

Silverâ†’Gold:

El `MERGE` deduplica automÃ¡ticamente por `(session_id, user_id)`.

**Datos corruptos:**

Cuando leemos JSON con Spark usamos modo PERMISSIVE con `columnNameOfCorruptRecord`. Los registros que no parsean se marcan y los mandamos a S3 quarantine:
```python
df_valid = df.filter(col("_corrupt_record").isNull())
df_corrupt = df.filter(col("_corrupt_record").isNotNull())

df_corrupt.write.json("s3://.../quarantine/events/")
```

Publicamos una mÃ©trica a CloudWatch con el count de registros corruptos. Si pasa de un threshold, se dispara una alarma a Slack.

Para rollback usamos Iceberg snapshots:
```sql
-- Ver historial
SELECT * FROM gold.user_session_analysis.snapshots;

-- Rollback si metimos datos malos
CALL system.rollback_to_snapshot('gold.user_session_analysis', 12345);
```

Esto no mueve datos, solo cambia el metadata pointer. Es instantÃ¡neo.

**Validaciones:**

Usamos Glue Data Quality rules antes de escribir a Silver:
- `IsUnique "event_id"`
- `IsComplete "user_id"`
- `ColumnValues "event_type" in ["page_view", "click", "purchase", ...]`
- `ColumnValues "session_duration_seconds" >= 0`

Si las reglas fallan, el job falla y se dispara la alerta. Preferimos fallar rÃ¡pido que propagar datos malos.

---

**Costos estimados:**
- Ingesta: ~$110/mes (Kinesis + Firehose + Lambdas)
- Storage: ~$15/mes (S3 con compresiÃ³n)
- Compute: ~$100/mes (Glue ETL)
- Red: ~$15/mes (VPC Endpoint)
- Misc: ~$10/mes (CloudWatch, Secrets Manager, DynamoDB)

**Total: ~$250/mes** para una plataforma de datos completa. Escalable y serverless.

# Parte 2: Prototipo

Prueba para el reto tÃ©cnico de Data Engineering.

## ğŸ“ Estructura del Proyecto
```
reto_kashio/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ generator/              # Generador de datos mock
â”‚   â”‚   â”œâ”€â”€ data_test/          # Datos generados
â”‚   â”‚   â”‚   â”œâ”€â”€ events.json
â”‚   â”‚   â”‚   â”œâ”€â”€ transactions.csv
â”‚   â”‚   â”‚   â””â”€â”€ users.csv
â”‚   â”‚   â”œâ”€â”€ config.ini          # ConfiguraciÃ³n del generador
â”‚   â”‚   â”œâ”€â”€ main_source.py      # Script principal
â”‚   â””â”€â”€ lakehouse/              # Data Lake local
â”‚       â”œâ”€â”€ bronze/             # Raw data
â”‚       â”œâ”€â”€ silver/             # Clean data (Iceberg)
â”‚       â””â”€â”€ gold/               # Analytics (Iceberg)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ spark_config.py     # ConfiguraciÃ³n Spark + Iceberg
â”‚   â”œâ”€â”€ jobs/
â”‚   â”‚   â”œâ”€â”€ bronze_to_silver_events.py
â”‚   â”‚   â”œâ”€â”€ bronze_to_silver_transactions.py
â”‚   â”‚   â”œâ”€â”€ bronze_to_silver_users.py
â”‚   â”‚   â””â”€â”€ silver_to_gold.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ logger.py           # Logger con colores
â”‚   â”‚   â”œâ”€â”€ data_quality.py     # Validaciones DQ
â”‚   â”‚   â””â”€â”€ iceberg_utils.py    # Helpers Iceberg
â”‚   â””â”€â”€ pipeline.py             # Orquestador
â”œâ”€â”€ tests/
â”œâ”€â”€ notebooks/
â”œâ”€â”€ jars/                       # Iceberg JAR
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ run_pipeline.sh             # Script de ejecuciÃ³n completo
```

## ğŸš€ InstalaciÃ³n y Uso

### Prerequisitos

- Python 3.8+
- Java 11+ (para PySpark)

### 1. Clonar repositorio
```bash
git clone 
cd reto_kashio
```
### 2. Crear ambiente virtual
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate  # Windows
```

### 3. Instalar dependencias
```bash
pip install -r requirements.txt
```

### 4. Descargar Iceberg JAR
```bash
mkdir -p jars
cd jars
wget https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.4.3/iceberg-spark-runtime-3.5_2.12-1.4.3.jar
cd ..
```

O descarga manual desde: [Maven Repository](https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.4.3/)


## ğŸ¯ EjecuciÃ³n

### OpciÃ³n 1: Pipeline completo (Recomendado)
```bash
chmod +x run_pipeline.sh
./run_pipeline.sh
```

Este script:
1. âœ… Genera datos mock con escenarios de calidad
2. âœ… Copia datos a Bronze layer
3. âœ… Ejecuta transformaciones Bronze â†’ Silver â†’ Gold
4. âœ… Valida resultados

### OpciÃ³n 2: Ejecutar por etapas

#### Generar datos mock
```bash
cd data/generator
python main_source.py
cd ../..
```

#### Copiar a Bronze
```bash
mkdir -p data/lakehouse/bronze/{events,transactions,users}
cp data/generator/data_test/events.json data/lakehouse/bronze/events/
cp data/generator/data_test/transactions.csv data/lakehouse/bronze/transactions/
cp data/generator/data_test/users.csv data/lakehouse/bronze/users/
```

#### Ejecutar transformaciones
```bash
# Bronze â†’ Silver
python src/jobs/bronze_to_silver_events.py
python src/jobs/bronze_to_silver_transactions.py
python src/jobs/bronze_to_silver_users.py

# Silver â†’ Gold
python src/jobs/silver_to_gold.py
```

#### Pipeline completo
```bash
python src/pipeline.py
```

## ğŸ“Š Generador de Datos Mock

### ConfiguraciÃ³n

Edita `data/generator/config.ini`:
```ini
[data_volumes]
num_users = 100          # NÃºmero de usuarios
num_sessions = 500       # NÃºmero de sesiones
num_events = 2000        # NÃºmero de eventos
num_transactions = 300   # NÃºmero de transacciones

[data_quality]
late_arrival_rate = 0.05  # 5% de eventos con retraso
duplicate_rate = 0.02     # 2% de duplicados
```

### CaracterÃ­sticas del generador

El generador crea datos realistas con:

âœ… **Late-arriving data**: 5% de eventos con timestamps retrasados (2-48 horas)  
âœ… **Duplicados**: 2% de transacciones duplicadas (mismo transaction_id)  
âœ… **Datos relacionados**: Eventos y transacciones vinculados a sesiones vÃ¡lidas  
âœ… **Variedad**: MÃºltiples tipos de eventos, paÃ­ses, dispositivos, monedas  

### Estructura de datos generados

#### Users (CSV)
```csv
user_id,signup_date,device_type,country
USR_00001,2024-03-15 14:23:00,iOS,PE
USR_00002,2024-05-20 09:15:00,Android,US
```

#### Events (JSON Lines)
```json
{"event_id": "EVT_00000001", "session_id": "SES_000001", "user_id": "USR_00001", "event_type": "page_view", "event_timestamp": "2024-11-15T14:30:00", "event_details": {"page_url": "/page/12"}}
```

#### Transactions (CSV)
```csv
transaction_id,session_id,user_id,amount,currency,transaction_timestamp
TXN_0000001,SES_000123,USR_00045,1250.50,USD,2024-11-15 14:45:00
```

## âœ¨ Features Implementadas

### Data Engineering
- âœ… Medallion Architecture (Bronze/Silver/Gold)
- âœ… Apache Iceberg (ACID transactions)
- âœ… Time Travel & Snapshots
- âœ… Schema Evolution
- âœ… Hidden Partitioning

### Data Quality
- âœ… DeduplicaciÃ³n automÃ¡tica
- âœ… ValidaciÃ³n de schemas
- âœ… Null checks
- âœ… Quarantine de datos corruptos
- âœ… Data Quality rules

### Pipeline Features
- âœ… MERGE idempotente (maneja late-arriving data)
- âœ… Logging con colores
- âœ… Modular y testeable
- âœ… Error handling robusto

## ğŸ“ JustificaciÃ³n TÃ©cnica

### Â¿Por quÃ© Iceberg?

- **ACID Transactions**: MERGE idempotente para late-arriving data
- **Time Travel**: Rollback instantÃ¡neo si metemos datos malos
- **Schema Evolution**: Agregar columnas sin reescribir tabla completa
- **Hidden Partitioning**: Usuarios no necesitan saber cÃ³mo estÃ¡ particionado

### Â¿Por quÃ© PySpark?

- **Escalabilidad**: Maneja millones de registros con joins complejos
- **Expresividad**: API SQL + DataFrame API
- **Ecosistema**: IntegraciÃ³n nativa con Iceberg, Parquet, Delta Lake

# Parte 3: Estrategia de IA/ML

## 1. Infraestructura para ML - DetecciÃ³n de Fraude en Tiempo Real

### Cambios Necesarios en la Arquitectura

#### A. Feature Store

**Problema:** 
La tabla Gold actualiza en batch. Para fraude real-time necesitamos features en milisegundos.

**SoluciÃ³n: Feature Store con dos capas**

- **Online Store (DynamoDB):** Features pre-calculadas por usuario, latencia <5ms, TTL 30 dÃ­as
- **Offline Store (Iceberg):** Ya lo tenemos, sirve para training e histÃ³rico

**Features clave:**
- Behavioral: sessions_24h, spent_24h, distinct_devices_24h, distinct_countries_24h
- Velocity: transactions_last_hour, amount_deviation_from_avg
- Account: days_since_signup, lifetime_value, chargeback_rate

**IntegraciÃ³n:**
- Batch: Glue ETL calcula desde Gold â†’ Offline Store (ya existe)
- Stream: Kinesis â†’ Lambda nueva â†’ Online Store (a construir)

#### B. Pipeline de Inferencia

**Flujo:**
```
Kinesis â†’ Lambda (get features) â†’ SageMaker Endpoint â†’ Decision
  - Score > 0.8: BLOCK + alerta
  - 0.5-0.8: REVIEW manual  
  - < 0.5: APPROVE
```

**Target: <200ms end-to-end**

**Componentes nuevos:**
- DynamoDB Online Store (~$25/mes)
- SageMaker Endpoint ml.t3.medium (~$50/mes)
- Lambda orchestration (~$10/mes)

#### C. QuÃ© reutilizamos vs quÃ© construimos

**Ya tenemos:**
- Kinesis (agregar consumer)
- Iceberg Gold (Offline Store)
- Glue ETL (feature engineering)

**A construir:**
- Branch del Kinesis para ML
- Online Store (DynamoDB)
- Lambda + SageMaker pipeline

**Costo aproximado:** ~$200/mes operacional

---

## 2. Uso de GenAI en Data Engineering

### Agente de Productividad Integrado

#### A. DocumentaciÃ³n AutomÃ¡tica

**Flujo:**
Pipeline crea tabla â†’ EventBridge â†’ Lambda â†’ Claude (Bedrock) â†’ Genera docs â†’ S3 + Glue Catalog

**Input al agente:**
- Schema de la tabla
- Sample data
- Contexto del pipeline

**Output:**
- Business description por columna
- Data dictionary
- Reglas de calidad recomendadas
- Queries de ejemplo


#### B. GeneraciÃ³n de Tests

El agente analiza schema + stats y genera pytest para:
- Schema compliance
- Rangos vÃ¡lidos
- Relaciones entre columnas
- Duplicados

#### C. NLP-to-SQL

**Caso:** Analista pregunta "Â¿CuÃ¡l es el paÃ­s con mayor conversiÃ³n en diciembre?"

**Agente:**
1. Identifica tabla y columnas necesarias
2. Genera SQL optimizado
3. Explica la query
4. Ejecuta en Athena
5. Visualiza en QuickSight

#### D. Debugging Inteligente

**Flujo:**
Job falla â†’ CloudWatch â†’ Lambda â†’ Claude analiza logs + config + schema â†’ Genera:
- Root cause
- Probable issue
- Fix sugerido
- Link a docs

Todo a Slack en 30 segundos.

---

### Impacto Medible

**ML Fraude:**
- Latencia: horas â†’ <200ms
- ROI: 1% fraude prevenido >> $200/mes infra
- Modelo mejora continuamente con feedback

**GenAI Productividad:**
- DocumentaciÃ³n: -100% tiempo manual
- Tests: -60% tiempo, mejor coverage
- Debugging: -50% tiempo
- DemocratizaciÃ³n: analistas autÃ³nomos

**Total: ~30% productividad ganada**

**Riesgos:**
- Costo APIs: rate limits + caching
- Calidad IA: human-in-the-loop para outputs crÃ­ticos
- Latencia ML: monitoreo + contingencia

**MÃ©tricas Ã©xito:**
- GenAI: -80% tiempo docs, >70% test coverage, -50% debugging
- ML: <200ms p99, <5% false positives, +30% fraude detectado
